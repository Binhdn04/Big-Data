{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp5DduDsD3FB"
      },
      "source": [
        "# Mapreduce with bash\n",
        "\n",
        "In this notebook we're going to use `bash` to write a mapper and a reducer to count words in a file. This example will serve to illustrate the main features of Hadoop's MapReduce framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsUM_0rlD3FF"
      },
      "source": [
        "# Table of contents\n",
        "- [What is MapReduce?](#mapreduce)\n",
        "- [The mapper](#mapper)\n",
        "    - [Test the mapper](#testmapper)\n",
        "- [Hadoop it up](#hadoop)\n",
        "    - [What is Hadoop Streaming?](#hadoopstreaming)\n",
        "    - [List your Hadoop directory](#hdfs_ls)\n",
        "    - [Test MapReduce with a dummy reducer](#dummyreducer)\n",
        "    - [Shuffling and sorting](#shuffling&sorting)\n",
        "- [The reducer](#reducer)\n",
        "    - [Test and run](#run)\n",
        "- [Run a mapreduce job with more data](#moredata)\n",
        "    - [Sort the output with `sort`](#sortoutput)\n",
        "    - [Sort the output with another MapReduce job](#sortoutputMR)\n",
        "    - [Configure sort with `KeyFieldBasedComparator`](#KeyFieldBasedComparator)\n",
        "    - [Specifying Configuration Variables with the -D Option](#configuration_variables)\n",
        "    - [What is word count useful for?](#wordcount)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H7GhxImD3FH"
      },
      "source": [
        "## What is MapReduce? <a name=\"mapreduce\"></a>\n",
        "\n",
        "MapReduce is a computing paradigm designed to allow parallel distributed processing of massive amounts of data.\n",
        "\n",
        "Data is split across several computer nodes, there it is processed by one or more mappers. The results emitted by the mappers are first sorted and then passed to one or more reducers that process and combine the data to return the final result.\n",
        "\n",
        "![Map & Reduce](https://github.com/hueptk0711/Big-Data/blob/main/MapReduce%20Tutorials/mapreduce.png?raw=1)\n",
        "With [Hadoop Streaming](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html) it is possible to use any programming language to define a mapper and/or a reducer. Here we're going to use the Unix `bash` scripting language ([here](https://www.gnu.org/software/bash/manual/html_node/index.html) is the official documentation for the language)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBN5dpwfD3FI"
      },
      "source": [
        "## The mapper <a name=\"mapper\"></a>\n",
        "Let's write a mapper script called `map.sh`. The mapper splits each input line into words and for each word it outputs a line containing the word and `1` separated by a tab.\n",
        "\n",
        "Example: for the input\n",
        "<html>\n",
        "<pre>\n",
        "apple orange\n",
        "banana apple peach\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "`map.sh` outputs:\n",
        "<html>\n",
        "<pre>\n",
        "apple   1\n",
        "orange  1\n",
        "banana  1\n",
        "apple  1\n",
        "peach  1\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "\n",
        "The <a href=\"https://ipython.readthedocs.io/en/stable/interactive/magics.html\">_cell magic_</a> [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) allows us to write the contents of the cell to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9YlfOSBD3FJ",
        "outputId": "ccf9fbe9-9856-48a7-f1d7-385687f11fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing map.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile map.sh\n",
        "#!/bin/bash\n",
        "\n",
        "while read line\n",
        "do\n",
        " for word in $line\n",
        " do\n",
        "  if [ -n \"$word\" ]\n",
        "  then\n",
        "     echo -e ${word}\"\\t1\"\n",
        "  fi\n",
        " done\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1jR1XrtD3FK"
      },
      "source": [
        "After running the cell above, you should have a new file `map.sh` in your current directory.\n",
        "The file can be seen in the left panel of JupyterLab or by using a list command on the bash command-line.\n",
        "\n",
        "**Note:** you can execute a single bash command in a Jupyter notebook cell by prepending an exclamation point to the command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbPU5lxuD3FK",
        "outputId": "31d06e90-7529-4eb4-cb5a-723aa14ebddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 124 May  6 14:12 map.sh\n"
          ]
        }
      ],
      "source": [
        "!ls -hl map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix9IGABpD3FL"
      },
      "source": [
        "### Test the mapper <a name=\"testmapper\"></a>\n",
        "We're going to test the mapper on on the command line with a small text file `fruits.txt` by first creating the text file.\n",
        "In this file `apple` for instance appears two times, that's what we want our mapreduce job to compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcgqEXw5D3FL",
        "outputId": "0277505a-e481-4e0a-c421-3b3abbad577e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fruits.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile fruits.txt\n",
        "apple banana\n",
        "peach orange peach peach\n",
        "pineapple peach apple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgOUZXxvD3FL",
        "outputId": "334c8aae-5d28-4302-97ff-2296c383bec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple banana\n",
            "peach orange peach peach\n",
            "pineapple peach apple\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQFCbVD7D3FM"
      },
      "source": [
        "Test the mapper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x map.sh"
      ],
      "metadata": {
        "id": "nBcalfBCELf7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vqlbtWND3FM",
        "outputId": "d72f167e-77c1-4720-dc2b-e14fbf80eac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple\t1\n",
            "banana\t1\n",
            "peach\t1\n",
            "orange\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "pineapple\t1\n",
            "peach\t1\n",
            "apple\t1\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt|./map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cGxPiebD3FN"
      },
      "source": [
        "If the script `map.sh` does not have the executable bit set, you need to set the correct permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f-LJFX_8D3FN"
      },
      "outputs": [],
      "source": [
        "!chmod 700 map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD_oCJq1D3FO"
      },
      "source": [
        "## Hadoop it up <a name=\"hadoop\"></a>\n",
        "Let us now run a MapReduce job with Hadoop Streaming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWr2LbuuGk8a",
        "outputId": "aae600d1-a218-4a94-940c-536578259b52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4I0stzyHFvZ",
        "outputId": "a5516440-f553-4379-b9bc-b2908e5ab26c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3_3ywvTHIlo",
        "outputId": "cbfe0361-35e2-4bfa-b917-9a7d8f2e22ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXsP40sHD3FO"
      },
      "source": [
        "### What is Hadoop Streaming <a name=\"hadoopstreaming\"></a>\n",
        "\n",
        "Hadoop Streaming is a library included in the Hadoop distribution that enables you to develop MapReduce executables in languages other than Java.\n",
        "\n",
        "Mapper and/or reducer can be any sort of executables that read the input from stdin and emit the output to stdout. By default, input is read line by line and the prefix of a line up to the first tab character is the key; the rest of the line (excluding the tab character) will be the value.\n",
        "\n",
        "If there is no tab character in the line, then the entire line is considered as key and the value is null. The default input format is specified in the class `TextInputFormat` (see the [API documentation](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html)) but this can can be customized for instance by defining another field separator (see the [Hadoop Streaming documentation](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Customizing_How_Lines_are_Split_into_KeyValue_Pairs).\n",
        "\n",
        "This is an example of MapReduce streaming invocation syntax:\n",
        "<html>\n",
        "<pre>\n",
        "    mapred streaming \\\n",
        "  -input myInputDirs \\\n",
        "  -output myOutputDir \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc\n",
        "\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "You can find the full official documentation for Hadoop Streaming from Apache Hadoop here: [https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html).\n",
        "\n",
        "All options for the Hadoop Streaming command are described here: [Streaming Command Options](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options) and can be listed with the command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB78UK9oD3FO",
        "outputId": "e1e27241-a92f-437a-a752-3c3175e56a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n"
          ]
        }
      ],
      "source": [
        "!mapred streaming --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9-801XPD3FO"
      },
      "source": [
        "Now in order to run a mapreduce job that we need to \"upload\" the input file to the Hadoop file system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_3qJpg_D3FO"
      },
      "source": [
        "### List your Hadoop directory <a name=\"hdfs_ls\"></a>\n",
        "\n",
        "With the command `hdfs dfs -l` you can view the content of your HDFS home directory.\n",
        "\n",
        "`hdfs dfs` you can run a filesystem command on the Hadoop fileystem. The complete list of commands can be found in the [System Shell Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdGlKZ_ND3FO",
        "outputId": "b08b63ee-e788-4623-da27-aa2a104359de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 items\n",
            "drwxr-xr-x   - root root       4096 2024-05-02 13:24 .config\n",
            "-rw-r--r--   1 root root         60 2024-05-06 14:12 fruits.txt\n",
            "drwxr-xr-x   - root root       4096 2024-03-04 08:05 hadoop-3.4.0\n",
            "-rw-r--r--   1 root root  965537117 2024-05-06 14:13 hadoop-3.4.0.tar.gz\n",
            "-rwx------   1 root root        124 2024-05-06 14:12 map.sh\n",
            "drwxr-xr-x   - root root       4096 2024-05-02 13:25 sample_data\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHanZjE3D3FP"
      },
      "source": [
        "Now create a directory `wordcount` with a subdirectory `input` on the Hadoop filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dmt_6ylDD3FP"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir -p wordcount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKXvmij-D3FP"
      },
      "source": [
        "Copy the file fruits.txt to Hadoop in the folder `wordcount/input`.\n",
        "\n",
        "Why do we need this step? Because the file `fruits.txt` needs to reside on the Hadoop filesystem in order to enjoy of all of the features of Hadoop (data partitioning, distributed processing, fault tolerance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GVqFURuyD3FP"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
        "hdfs dfs -mkdir wordcount/input\n",
        "hdfs dfs -put fruits.txt wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qucg8xRRD3FP"
      },
      "source": [
        "Let's check if the file is there now.\n",
        "\n",
        "**Note:** it is convenient use the option `-h` for `ls` to show file sizes in human-readable form (showing sizes in Kilobytes, Megabytes, Gigabytes, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOtBLiJCD3FP",
        "outputId": "bc10814f-49c8-4486-d677-a05f040534f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r--   1 root root         60 2024-05-06 14:14 wordcount/input/fruits.txt\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls -h -R wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUrZeJN5D3FP"
      },
      "source": [
        "### Test MapReduce with a dummy reducer <a name=\"dummyreducer\"></a>\n",
        "\n",
        "Let's try to run the mapper using a dummy reducer (`/bin/cat` does nothing else than echoing the data it receives).\n",
        "\n",
        "**Warning:** mapreduce tends to produce a verbose output, so be ready to see a long output. What you should look for is a message of the kind <html><pre>\"INFO mapreduce.Job: Job ... completed successfully\"</pre></html>\n",
        "\n",
        "**Note:** at the beginning of next cell you'll see a command `hadoop fs -rmr wordcount/output 2>/dev/null`. This is needed because when you run a job several times mapreduce will give an error if you try to overwrite the same output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut5tsrcED3FQ",
        "outputId": "6efba71e-7941-4c67-dbe7-32850c7efc79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-06 14:14:11,505 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 14:14:11,778 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 14:14:11,778 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 14:14:11,806 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:14:12,225 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 14:14:12,251 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 14:14:12,744 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1979356913_0001\n",
            "2024-05-06 14:14:12,744 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 14:14:13,190 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local1979356913_0001_778a14ab-a892-46c1-ab14-d63f33e4bcb2/map.sh\n",
            "2024-05-06 14:14:13,366 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 14:14:13,369 INFO mapreduce.Job: Running job: job_local1979356913_0001\n",
            "2024-05-06 14:14:13,401 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 14:14:13,405 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 14:14:13,411 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:13,412 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:13,517 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 14:14:13,522 INFO mapred.LocalJobRunner: Starting task: attempt_local1979356913_0001_m_000000_0\n",
            "2024-05-06 14:14:13,563 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:13,564 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:13,598 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:14:13,614 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n",
            "2024-05-06 14:14:13,632 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-05-06 14:14:13,708 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-06 14:14:13,708 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-06 14:14:13,708 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-06 14:14:13,709 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-06 14:14:13,709 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-06 14:14:13,712 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-06 14:14:13,722 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-05-06 14:14:13,729 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-05-06 14:14:13,734 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-05-06 14:14:13,735 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-05-06 14:14:13,735 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-05-06 14:14:13,736 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-05-06 14:14:13,737 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-05-06 14:14:13,739 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-05-06 14:14:13,740 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-05-06 14:14:13,740 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-05-06 14:14:13,741 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-05-06 14:14:13,742 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-06 14:14:13,743 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-05-06 14:14:13,768 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:13,770 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 14:14:13,776 INFO streaming.PipeMapRed: Records R/W=3/1\n",
            "2024-05-06 14:14:13,777 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 14:14:13,781 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 14:14:13,781 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-06 14:14:13,781 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-06 14:14:13,781 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n",
            "2024-05-06 14:14:13,781 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n",
            "2024-05-06 14:14:13,791 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-06 14:14:13,805 INFO mapred.Task: Task:attempt_local1979356913_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:14:13,811 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
            "2024-05-06 14:14:13,811 INFO mapred.Task: Task 'attempt_local1979356913_0001_m_000000_0' done.\n",
            "2024-05-06 14:14:13,823 INFO mapred.Task: Final Counters for attempt_local1979356913_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142117\n",
            "\t\tFILE: Number of bytes written=861811\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=9\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=383778816\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "2024-05-06 14:14:13,823 INFO mapred.LocalJobRunner: Finishing task: attempt_local1979356913_0001_m_000000_0\n",
            "2024-05-06 14:14:13,823 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 14:14:13,828 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-06 14:14:13,830 INFO mapred.LocalJobRunner: Starting task: attempt_local1979356913_0001_r_000000_0\n",
            "2024-05-06 14:14:13,841 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:13,841 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:13,842 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:14:13,850 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@38029bf7\n",
            "2024-05-06 14:14:13,852 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:14:13,893 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-06 14:14:13,899 INFO reduce.EventFetcher: attempt_local1979356913_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-06 14:14:13,964 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1979356913_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n",
            "2024-05-06 14:14:13,969 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local1979356913_0001_m_000000_0\n",
            "2024-05-06 14:14:13,975 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n",
            "2024-05-06 14:14:13,978 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-06 14:14:13,980 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:13,980 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-06 14:14:13,987 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:14:13,987 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-05-06 14:14:13,989 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-06 14:14:13,990 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n",
            "2024-05-06 14:14:13,991 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-06 14:14:13,991 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:14:13,992 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-05-06 14:14:13,994 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:13,995 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-05-06 14:14:13,998 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-05-06 14:14:14,002 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-05-06 14:14:14,019 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:14,022 INFO streaming.PipeMapRed: Records R/W=9/1\n",
            "2024-05-06 14:14:14,023 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 14:14:14,023 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 14:14:14,025 INFO mapred.Task: Task:attempt_local1979356913_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:14:14,029 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:14,029 INFO mapred.Task: Task attempt_local1979356913_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-06 14:14:14,033 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1979356913_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-05-06 14:14:14,035 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
            "2024-05-06 14:14:14,035 INFO mapred.Task: Task 'attempt_local1979356913_0001_r_000000_0' done.\n",
            "2024-05-06 14:14:14,036 INFO mapred.Task: Final Counters for attempt_local1979356913_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142353\n",
            "\t\tFILE: Number of bytes written=862003\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=9\n",
            "\t\tSpilled Records=9\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=6\n",
            "\t\tTotal committed heap usage (bytes)=383778816\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=90\n",
            "2024-05-06 14:14:14,036 INFO mapred.LocalJobRunner: Finishing task: attempt_local1979356913_0001_r_000000_0\n",
            "2024-05-06 14:14:14,036 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-06 14:14:14,400 INFO mapreduce.Job: Job job_local1979356913_0001 running in uber mode : false\n",
            "2024-05-06 14:14:14,401 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-06 14:14:14,402 INFO mapreduce.Job: Job job_local1979356913_0001 completed successfully\n",
            "2024-05-06 14:14:14,415 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=284470\n",
            "\t\tFILE: Number of bytes written=1723814\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=9\n",
            "\t\tSpilled Records=18\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=6\n",
            "\t\tTotal committed heap usage (bytes)=767557632\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=90\n",
            "2024-05-06 14:14:14,415 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -files map.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer /bin/cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiflR496D3FR"
      },
      "source": [
        "The output of the mapreduce job is in the `output` subfolder of the input directory. Let's check what's inside it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXE5elH9D3FR",
        "outputId": "7320d5e5-0153-4d6c-f8d6-e7d9af54d582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-05-06 14:14 wordcount/output/_SUCCESS\n",
            "-rw-r--r--   1 root root         78 2024-05-06 14:14 wordcount/output/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSVTe1_jD3FR"
      },
      "source": [
        "If `output` contains a file named `_SUCCESS` that means that the mapreduce job completed successfully.\n",
        "\n",
        "**Note:** when dealing with Big Data it's always advisable to pipe the output of `cat` commands to `head` (or `tail`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOiDVpQDD3FR",
        "outputId": "f241d974-aa77-4cf0-e4b6-0bfa9dc178b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple\t1\n",
            "apple\t1\n",
            "banana\t1\n",
            "orange\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "pineapple\t1\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part*|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOpW1oAcD3FS"
      },
      "source": [
        "We have gotten as expected all the output from the mapper. Something worth of notice is that the data outputted from the mapper _**has been sorted**_. We haven't asked for that but this step is automatically performed by the mapper as soon as the number of reducers is $\\gt 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFVuyTAD3FS"
      },
      "source": [
        "### Shuffling and sorting <a name=\"shuffling&sorting\"></a>\n",
        "The following picture illustrates the concept of shuffling and sorting that is automatically performed by Hadoop after each map before passing the output to reduce. In the picture the outputs of the two mapper tasks are shown. The arrows represent shuffling and sorting done before delivering the data to one reducer (rightmost box).\n",
        "![Shuffle & sort](https://github.com/hueptk0711/Big-Data/blob/main/MapReduce%20Tutorials/shuffle_sort.png?raw=1)\n",
        "The shuffling and sorting phase is often one of the most costly in a MapReduce job.\n",
        "\n",
        "\n",
        "<b>Note:</b> the job ran with two mappers because $2$ is the default number of mappers in Hadoop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OLEKdNMD3FS"
      },
      "source": [
        "## The reducer <a name=\"reducer\"></a>\n",
        "Let's now write a reducer script called `reduce.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeXiCVmDD3FS",
        "outputId": "d463ebd3-a1f8-4062-9075-cbbd92c39500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduce.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile reduce.sh\n",
        "#!/bin/bash\n",
        "\n",
        "currkey=\"\"\n",
        "currcount=0\n",
        "while IFS=$'\\t' read -r key val\n",
        "do\n",
        "  if [[ $key == $currkey ]]\n",
        "  then\n",
        "      currcount=$(( currcount + val ))\n",
        "  else\n",
        "    if [ -n \"$currkey\" ]\n",
        "    then\n",
        "      echo -e ${currkey} \"\\t\" ${currcount}\n",
        "    fi\n",
        "    currkey=$key\n",
        "    currcount=1\n",
        "  fi\n",
        "done\n",
        "# last one\n",
        "echo -e ${currkey} \"\\t\" ${currcount}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_inmhPDGD3FS"
      },
      "source": [
        "Set permission for the reducer script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fqA0Ag2yD3FS"
      },
      "outputs": [],
      "source": [
        "!chmod 700 reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaD2UV0QD3FT"
      },
      "source": [
        "### Test and run <a name=\"run\"></a>\n",
        "\n",
        "Test map and reduce on the shell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49VknYKbD3FU",
        "outputId": "bcd7b8ea-136d-49f5-d9c7-8574c134a994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple \t 2\n",
            "banana \t 1\n",
            "orange \t 1\n",
            "peach \t 4\n",
            "pineapple \t 1\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt|./map.sh|sort|./reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m05eooyDD3FU"
      },
      "source": [
        "Once we've made sure that the reducer script runs correctly on the shell, we can run it on the cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX_In5JID3FU",
        "outputId": "1559ccac-998b-44a4-92d9-9a007be17cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output\n",
            "packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob1788553120462534689.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-06 14:14:23,741 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-05-06 14:14:24,726 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 14:14:24,943 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 14:14:24,943 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 14:14:24,974 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:14:25,253 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 14:14:25,283 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 14:14:25,654 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local66711868_0001\n",
            "2024-05-06 14:14:25,654 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 14:14:26,143 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local66711868_0001_1b98066b-0061-48a7-9491-5ac21538791e/map.sh\n",
            "2024-05-06 14:14:26,179 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local66711868_0001_1af21d7d-06b9-4408-a5fc-aead1020752b/reduce.sh\n",
            "2024-05-06 14:14:26,349 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 14:14:26,352 INFO mapreduce.Job: Running job: job_local66711868_0001\n",
            "2024-05-06 14:14:26,359 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 14:14:26,361 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 14:14:26,384 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:26,384 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:26,449 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 14:14:26,453 INFO mapred.LocalJobRunner: Starting task: attempt_local66711868_0001_m_000000_0\n",
            "2024-05-06 14:14:26,489 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:26,492 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:26,523 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:14:26,534 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n",
            "2024-05-06 14:14:26,554 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-05-06 14:14:26,630 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-06 14:14:26,630 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-06 14:14:26,630 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-06 14:14:26,630 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-06 14:14:26,630 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-06 14:14:26,635 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-06 14:14:26,644 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-05-06 14:14:26,653 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-05-06 14:14:26,656 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-05-06 14:14:26,657 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-05-06 14:14:26,658 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-05-06 14:14:26,658 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-05-06 14:14:26,659 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-05-06 14:14:26,661 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-05-06 14:14:26,661 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-05-06 14:14:26,662 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-05-06 14:14:26,662 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-05-06 14:14:26,663 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-06 14:14:26,664 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-05-06 14:14:26,695 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:26,698 INFO streaming.PipeMapRed: Records R/W=3/1\n",
            "2024-05-06 14:14:26,698 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 14:14:26,699 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 14:14:26,704 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 14:14:26,704 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-06 14:14:26,704 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-06 14:14:26,704 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n",
            "2024-05-06 14:14:26,704 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n",
            "2024-05-06 14:14:26,714 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-06 14:14:26,734 INFO mapred.Task: Task:attempt_local66711868_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:14:26,737 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
            "2024-05-06 14:14:26,738 INFO mapred.Task: Task 'attempt_local66711868_0001_m_000000_0' done.\n",
            "2024-05-06 14:14:26,748 INFO mapred.Task: Final Counters for attempt_local66711868_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1193\n",
            "\t\tFILE: Number of bytes written=713084\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=9\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=376438784\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "2024-05-06 14:14:26,748 INFO mapred.LocalJobRunner: Finishing task: attempt_local66711868_0001_m_000000_0\n",
            "2024-05-06 14:14:26,749 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 14:14:26,754 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-06 14:14:26,754 INFO mapred.LocalJobRunner: Starting task: attempt_local66711868_0001_r_000000_0\n",
            "2024-05-06 14:14:26,768 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:26,768 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:26,768 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:14:26,782 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@31c2fa80\n",
            "2024-05-06 14:14:26,788 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:14:26,824 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-06 14:14:26,829 INFO reduce.EventFetcher: attempt_local66711868_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-06 14:14:26,894 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local66711868_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n",
            "2024-05-06 14:14:26,898 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local66711868_0001_m_000000_0\n",
            "2024-05-06 14:14:26,902 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n",
            "2024-05-06 14:14:26,905 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-06 14:14:26,907 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:26,907 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-06 14:14:26,915 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:14:26,915 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-05-06 14:14:26,917 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-06 14:14:26,919 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n",
            "2024-05-06 14:14:26,919 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-06 14:14:26,919 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:14:26,920 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-05-06 14:14:26,921 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:26,928 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n",
            "2024-05-06 14:14:26,932 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-05-06 14:14:26,936 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-05-06 14:14:26,961 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:26,966 INFO streaming.PipeMapRed: Records R/W=9/1\n",
            "2024-05-06 14:14:26,967 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 14:14:26,968 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 14:14:26,969 INFO mapred.Task: Task:attempt_local66711868_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:14:26,971 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:26,971 INFO mapred.Task: Task attempt_local66711868_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-06 14:14:26,973 INFO output.FileOutputCommitter: Saved output of task 'attempt_local66711868_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-05-06 14:14:26,974 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
            "2024-05-06 14:14:26,974 INFO mapred.Task: Task 'attempt_local66711868_0001_r_000000_0' done.\n",
            "2024-05-06 14:14:26,975 INFO mapred.Task: Final Counters for attempt_local66711868_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1429\n",
            "\t\tFILE: Number of bytes written=713254\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=9\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=376438784\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=68\n",
            "2024-05-06 14:14:26,975 INFO mapred.LocalJobRunner: Finishing task: attempt_local66711868_0001_r_000000_0\n",
            "2024-05-06 14:14:26,975 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-06 14:14:27,357 INFO mapreduce.Job: Job job_local66711868_0001 running in uber mode : false\n",
            "2024-05-06 14:14:27,358 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-06 14:14:27,360 INFO mapreduce.Job: Job job_local66711868_0001 completed successfully\n",
            "2024-05-06 14:14:27,369 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2622\n",
            "\t\tFILE: Number of bytes written=1426338\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=18\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=752877568\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=68\n",
            "2024-05-06 14:14:27,369 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file map.sh \\\n",
        "  -file reduce.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4rVxTFiD3FV"
      },
      "source": [
        "Let's check the output on the HDFS filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFa3o02jD3FV",
        "outputId": "296e1c00-cce1-4169-8a7b-8c049503376e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple \t 2\n",
            "banana \t 1\n",
            "orange \t 1\n",
            "peach \t 4\n",
            "pineapple \t 1\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part*|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COUcf-XOD3FW"
      },
      "source": [
        "## Run a mapreduce job with more data <a name=\"moredata\"></a>\n",
        "\n",
        "Let's create a datafile by downloading some real data, for instance from a Web page. This example will be used to introduce some advanced configurations.\n",
        "\n",
        "Next, we download a URL with `wget` and filter out HTML tags with a `sed` regular expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MdvWEhzBD3FW"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "URL=https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer\n",
        "wget -qO- $URL | sed -e 's/<[^>]*>//g;s/^ //g' >sample_article.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmW-whJ6D3FW",
        "outputId": "9997c4fd-52a2-40c4-b813-2f1bcaec0465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "Und\t1\n",
            "wo\t1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOaIENfWD3FX"
      },
      "source": [
        "As usual, with real data there's some more work to do. Here we see that the mapper script doesn't skip empty lines. Let's modify it so that empty lines are skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uljg1_teD3FX",
        "outputId": "07e50b20-9565-440d-8eec-2ef15bdcdffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting map.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile map.sh\n",
        "#!/bin/bash\n",
        "\n",
        "while read line\n",
        "do\n",
        " for word in $line\n",
        " do\n",
        "  if [[ \"$line\" =~ [^[:space:]] ]]\n",
        "  then\n",
        "    if [ -n \"$word\" ]\n",
        "    then\n",
        "    echo -e ${word} \"\\t1\"\n",
        "    fi\n",
        "  fi\n",
        " done\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnp1fwzxD3FY",
        "outputId": "95567082-7777-41ae-d506-a51cc7cf0770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t1\n",
            "Und \t1\n",
            "wo \t1\n",
            "warst \t1\n",
            "du \t1\n",
            "beim \t1\n",
            "Fall \t1\n",
            "der \t1\n",
            "Mauer? \t1\n",
            "- \t1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvb0h_hDD3FY"
      },
      "source": [
        "Now the output of `map.sh` looks better!\n",
        "\n",
        "<b>Note:</b> when working with real data we need in general some more preprocessing in order to remove control characters or invalid unicode.\n",
        "\n",
        "Time to run MapReduce again with the new data, but first we need to \"put\" the data on HDFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB6QXbf0D3FY",
        "outputId": "fa0bf88d-4b0f-41b3-aece-ae8eb2d76017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/input\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
        "hdfs dfs -put sample_article.txt wordcount/input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk9bvaioD3FZ",
        "outputId": "791210bc-b7de-41c1-9464-563c3236bafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r--   1 root root     29.0 K 2024-05-06 14:14 wordcount/input\n"
          ]
        }
      ],
      "source": [
        "# check that the folder wordcount/input on HDFS only contains sample_article.txt\n",
        "!hdfs dfs -ls -h wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XvAhcSuD3Fa"
      },
      "source": [
        "Check the reducer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oYQDdc7D3Fa",
        "outputId": "d1db1267-5abf-4e68-8aba-628f2a30f68b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t 1\n",
            "Und \t 1\n",
            "wo \t 1\n",
            "warst \t 1\n",
            "du \t 1\n",
            "beim \t 1\n",
            "Fall \t 1\n",
            "der \t 1\n",
            "Mauer? \t 1\n",
            "- \t 1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|./reduce.sh|head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_xSCtZ0D3Fa",
        "outputId": "542011b9-3469-49a4-9fba-7e439e2a32c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output\n",
            "packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob5471666965070249423.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-06 14:14:41,174 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-05-06 14:14:42,425 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 14:14:42,831 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 14:14:42,834 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 14:14:42,896 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:14:43,364 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 14:14:43,422 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 14:14:44,041 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local42192596_0001\n",
            "2024-05-06 14:14:44,041 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 14:14:44,757 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local42192596_0001_a6959183-b5fd-465b-88ca-c876e983b2dd/map.sh\n",
            "2024-05-06 14:14:44,798 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local42192596_0001_abc6a00d-b63d-495f-a2ed-8bf5bdb015bb/reduce.sh\n",
            "2024-05-06 14:14:44,970 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 14:14:44,973 INFO mapreduce.Job: Running job: job_local42192596_0001\n",
            "2024-05-06 14:14:44,979 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 14:14:44,981 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 14:14:44,987 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:44,987 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:45,050 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 14:14:45,055 INFO mapred.LocalJobRunner: Starting task: attempt_local42192596_0001_m_000000_0\n",
            "2024-05-06 14:14:45,094 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:45,096 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:45,131 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:14:45,145 INFO mapred.MapTask: Processing split: file:/content/wordcount/input:0+29720\n",
            "2024-05-06 14:14:45,167 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-05-06 14:14:45,238 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-06 14:14:45,238 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-06 14:14:45,238 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-06 14:14:45,238 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-06 14:14:45,238 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-06 14:14:45,242 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-06 14:14:45,249 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-05-06 14:14:45,262 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-05-06 14:14:45,263 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-05-06 14:14:45,263 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-05-06 14:14:45,264 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-05-06 14:14:45,265 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-05-06 14:14:45,265 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-05-06 14:14:45,270 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-05-06 14:14:45,270 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-05-06 14:14:45,271 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-05-06 14:14:45,271 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-05-06 14:14:45,272 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-06 14:14:45,273 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-05-06 14:14:45,293 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:45,293 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:45,295 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:45,309 INFO streaming.PipeMapRed: Records R/W=186/1\n",
            "2024-05-06 14:14:45,594 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 14:14:45,595 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 14:14:45,598 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 14:14:45,598 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-06 14:14:45,598 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-06 14:14:45,598 INFO mapred.MapTask: bufstart = 0; bufend = 32527; bufvoid = 104857600\n",
            "2024-05-06 14:14:45,598 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26209816(104839264); length = 4581/6553600\n",
            "2024-05-06 14:14:45,634 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-06 14:14:45,651 INFO mapred.Task: Task:attempt_local42192596_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:14:45,654 INFO mapred.LocalJobRunner: Records R/W=186/1\n",
            "2024-05-06 14:14:45,654 INFO mapred.Task: Task 'attempt_local42192596_0001_m_000000_0' done.\n",
            "2024-05-06 14:14:45,687 INFO mapred.Task: Final Counters for attempt_local42192596_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=31159\n",
            "\t\tFILE: Number of bytes written=747929\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=186\n",
            "\t\tMap output records=1146\n",
            "\t\tMap output bytes=32527\n",
            "\t\tMap output materialized bytes=34873\n",
            "\t\tInput split bytes=81\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1146\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=353370112\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29968\n",
            "2024-05-06 14:14:45,687 INFO mapred.LocalJobRunner: Finishing task: attempt_local42192596_0001_m_000000_0\n",
            "2024-05-06 14:14:45,687 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 14:14:45,698 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-06 14:14:45,698 INFO mapred.LocalJobRunner: Starting task: attempt_local42192596_0001_r_000000_0\n",
            "2024-05-06 14:14:45,728 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:14:45,728 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:14:45,728 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:14:45,738 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1150bff6\n",
            "2024-05-06 14:14:45,740 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:14:45,770 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-06 14:14:45,778 INFO reduce.EventFetcher: attempt_local42192596_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-06 14:14:45,831 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local42192596_0001_m_000000_0 decomp: 34869 len: 34873 to MEMORY\n",
            "2024-05-06 14:14:45,836 INFO reduce.InMemoryMapOutput: Read 34869 bytes from map-output for attempt_local42192596_0001_m_000000_0\n",
            "2024-05-06 14:14:45,841 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 34869, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34869\n",
            "2024-05-06 14:14:45,844 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-06 14:14:45,846 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:45,846 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-06 14:14:45,858 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:14:45,858 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n",
            "2024-05-06 14:14:45,880 INFO reduce.MergeManagerImpl: Merged 1 segments, 34869 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-06 14:14:45,881 INFO reduce.MergeManagerImpl: Merging 1 files, 34873 bytes from disk\n",
            "2024-05-06 14:14:45,882 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-06 14:14:45,882 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:14:45,884 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n",
            "2024-05-06 14:14:45,886 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:45,900 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n",
            "2024-05-06 14:14:45,905 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-05-06 14:14:45,908 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-05-06 14:14:45,936 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:45,936 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:45,938 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:45,958 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:14:45,970 INFO streaming.PipeMapRed: Records R/W=1146/1\n",
            "2024-05-06 14:14:45,978 INFO mapreduce.Job: Job job_local42192596_0001 running in uber mode : false\n",
            "2024-05-06 14:14:45,979 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-05-06 14:14:46,048 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 14:14:46,049 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 14:14:46,052 INFO mapred.Task: Task:attempt_local42192596_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:14:46,053 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:14:46,054 INFO mapred.Task: Task attempt_local42192596_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-06 14:14:46,056 INFO output.FileOutputCommitter: Saved output of task 'attempt_local42192596_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-05-06 14:14:46,057 INFO mapred.LocalJobRunner: Records R/W=1146/1 > reduce\n",
            "2024-05-06 14:14:46,057 INFO mapred.Task: Task 'attempt_local42192596_0001_r_000000_0' done.\n",
            "2024-05-06 14:14:46,058 INFO mapred.Task: Final Counters for attempt_local42192596_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=100937\n",
            "\t\tFILE: Number of bytes written=812394\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=736\n",
            "\t\tReduce shuffle bytes=34873\n",
            "\t\tReduce input records=1146\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1146\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=353370112\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=29592\n",
            "2024-05-06 14:14:46,058 INFO mapred.LocalJobRunner: Finishing task: attempt_local42192596_0001_r_000000_0\n",
            "2024-05-06 14:14:46,058 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-06 14:14:46,981 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-06 14:14:46,981 INFO mapreduce.Job: Job job_local42192596_0001 completed successfully\n",
            "2024-05-06 14:14:46,993 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=132096\n",
            "\t\tFILE: Number of bytes written=1560323\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=186\n",
            "\t\tMap output records=1146\n",
            "\t\tMap output bytes=32527\n",
            "\t\tMap output materialized bytes=34873\n",
            "\t\tInput split bytes=81\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=736\n",
            "\t\tReduce shuffle bytes=34873\n",
            "\t\tReduce input records=1146\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=2292\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=706740224\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29968\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=29592\n",
            "2024-05-06 14:14:46,993 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hadoop fs -rmr wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file map.sh \\\n",
        "  -file reduce.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWl5JZ8wD3Fb"
      },
      "source": [
        "Check the output on HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh0m4Zp8D3Fb",
        "outputId": "8c2e4e82-0d7d-40e6-805d-43bbaa564c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-05-06 14:14 wordcount/output/_SUCCESS\n",
            "-rw-r--r--   1 root root      29352 2024-05-06 14:14 wordcount/output/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70a5eU1pD3Fb"
      },
      "source": [
        "This job took a few seconds and this is quite some time for such a small file (4KB). This is due to the overhead of distributing the data and running the Hadoop framework.\n",
        "The advantage of Hadoop can be appreciated only for large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ls0Ui7hD3Fb",
        "outputId": "200fdd12-bcda-45cc-c116-bb24f4665a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!n.frames[t]; \t 1\n",
            "!0)); \t 1\n",
            "!1)) \t 1\n",
            "!1, \t 1\n",
            "!= \t 1\n",
            "!== \t 2\n",
            "!function \t 2\n",
            "!r \t 1\n",
            "\"'+n+'\"',o)}return{key:r,value:e.substr(t+1)}},t._renewCache=function(){t._cache=t._getCacheFromString(t._document.cookie),t._cachedDocumentCookie=t._document.cookie},t._areEnabled=function(){var \t 1\n",
            "\"))}function \t 1\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvt3S7uWD3Fb"
      },
      "source": [
        "### Sort the output with `sort` <a name=\"sortoutput\"></a>\n",
        "\n",
        "We've obtained a list of tokens that appear in the file followed by their frequencies.\n",
        "\n",
        "The output of the reducer is sorted by key (the word) because that's the ordering that the reducer becomes from the mapper. If we're interested in sorting the data by frequency, we can use the Unix `sort` command (with the options `k2`, `n`, `r` respectively \"by field 2\", \"numeric\", \"reverse\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBq1abi8D3Fc",
        "outputId": "120ea2c1-f947-478a-e820-5a9900944017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= \t 40\n",
            "{ \t 22\n",
            "var \t 22\n",
            "&& \t 19\n",
            "strict\";function \t 13\n",
            "} \t 12\n",
            "in \t 12\n",
            "not \t 12\n",
            "to \t 10\n",
            "e&&e.__esModule?e:{\"default\":e}}function \t 9\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part-00000|sort -k2nr|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M89ZZizrD3Fc"
      },
      "source": [
        "The most common word appears to be \"die\" (the German for the definite article \"the\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyAIHjQdD3Fc"
      },
      "source": [
        "### Sort the output with another MapReduce job <a name=\"sortoutputMR\"></a>\n",
        "\n",
        "If we wanted to sort the output of the reducer using the mapreduce framework, we could employ a simple trick: create a mapper that interchanges words with their frequency values. Since by construction mappers sort their output by key, we get the desired sorting as a side-effect.\n",
        "\n",
        "Call the new mapper `swap_keyval.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVoMDI8MD3Fc",
        "outputId": "a059add0-870d-4c3f-eced-a62e44b8523c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing swap_keyval.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile swap_keyval.sh\n",
        "#!/bin/bash\n",
        "# This script will read one line at a time and swap key/value\n",
        "# For instance, the line \"word 100\" will become \"100 word\"\n",
        "\n",
        "while read key val\n",
        "do\n",
        " printf \"%s\\t%s\\n\" \"$val\" \"$key\"\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLkYiRvHD3Fd"
      },
      "source": [
        "We are going to run the swap mapper script on the output of the previous mapreduce job. Note that in the below cell we are not deleting the previous output but instead we're saving the output from the current job in a new folder `output_sorted`.\n",
        "\n",
        "Nice thing about running a job on the output of a preceding job is that we do not need to upload files to HDFS because the data is already on HDFS. Not so nice: writing data to disk at each step of a data transformation pipeline takes time and this can be costly for longer data pipelines. This is one of the shortcomings of MapReduce that are addressed by [Apache Spark](https://spark.apache.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhpbRr5HD3Fd",
        "outputId": "2a42ab33-7c2c-424c-a3f9-bb3ba43e0241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [swap_keyval.sh] [] /tmp/streamjob7483636840258024631.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-06 14:14:58,204 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-05-06 14:14:59,131 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 14:14:59,335 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 14:14:59,335 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 14:14:59,365 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:14:59,692 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 14:14:59,726 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 14:15:00,108 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1256038990_0001\n",
            "2024-05-06 14:15:00,108 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 14:15:00,618 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local1256038990_0001_86378e5c-6771-45ed-8cb8-c363c9da965e/swap_keyval.sh\n",
            "2024-05-06 14:15:00,771 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 14:15:00,774 INFO mapreduce.Job: Running job: job_local1256038990_0001\n",
            "2024-05-06 14:15:00,781 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 14:15:00,784 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 14:15:00,793 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:15:00,793 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:15:00,881 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 14:15:00,886 INFO mapred.LocalJobRunner: Starting task: attempt_local1256038990_0001_m_000000_0\n",
            "2024-05-06 14:15:00,930 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:15:00,934 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:15:00,962 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:15:00,980 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n",
            "2024-05-06 14:15:01,003 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-05-06 14:15:01,089 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-06 14:15:01,089 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-06 14:15:01,089 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-06 14:15:01,089 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-06 14:15:01,090 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-06 14:15:01,093 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-06 14:15:01,106 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n",
            "2024-05-06 14:15:01,113 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-05-06 14:15:01,115 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-05-06 14:15:01,116 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-05-06 14:15:01,116 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-05-06 14:15:01,117 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-05-06 14:15:01,118 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-05-06 14:15:01,120 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-05-06 14:15:01,121 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-05-06 14:15:01,121 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-05-06 14:15:01,122 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-05-06 14:15:01,123 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-06 14:15:01,124 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-05-06 14:15:01,157 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:15:01,157 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:15:01,161 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:15:01,177 INFO streaming.PipeMapRed: Records R/W=746/1\n",
            "2024-05-06 14:15:01,215 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 14:15:01,216 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 14:15:01,220 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 14:15:01,220 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-06 14:15:01,220 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-06 14:15:01,220 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n",
            "2024-05-06 14:15:01,220 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n",
            "2024-05-06 14:15:01,246 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-06 14:15:01,262 INFO mapred.Task: Task:attempt_local1256038990_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:15:01,266 INFO mapred.LocalJobRunner: Records R/W=746/1\n",
            "2024-05-06 14:15:01,266 INFO mapred.Task: Task 'attempt_local1256038990_0001_m_000000_0' done.\n",
            "2024-05-06 14:15:01,300 INFO mapred.Task: Final Counters for attempt_local1256038990_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=30232\n",
            "\t\tFILE: Number of bytes written=747009\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=746\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=371195904\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "2024-05-06 14:15:01,301 INFO mapred.LocalJobRunner: Finishing task: attempt_local1256038990_0001_m_000000_0\n",
            "2024-05-06 14:15:01,301 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 14:15:01,314 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-06 14:15:01,315 INFO mapred.LocalJobRunner: Starting task: attempt_local1256038990_0001_r_000000_0\n",
            "2024-05-06 14:15:01,348 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:15:01,348 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:15:01,349 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:15:01,355 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@573c29b2\n",
            "2024-05-06 14:15:01,359 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:15:01,387 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-06 14:15:01,390 INFO reduce.EventFetcher: attempt_local1256038990_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-06 14:15:01,445 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1256038990_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n",
            "2024-05-06 14:15:01,449 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local1256038990_0001_m_000000_0\n",
            "2024-05-06 14:15:01,455 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n",
            "2024-05-06 14:15:01,460 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-06 14:15:01,462 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:15:01,462 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-06 14:15:01,472 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:15:01,472 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n",
            "2024-05-06 14:15:01,483 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-06 14:15:01,484 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n",
            "2024-05-06 14:15:01,485 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-06 14:15:01,485 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:15:01,486 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n",
            "2024-05-06 14:15:01,487 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:15:01,529 INFO mapred.Task: Task:attempt_local1256038990_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:15:01,531 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:15:01,531 INFO mapred.Task: Task attempt_local1256038990_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-06 14:15:01,533 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1256038990_0001_r_000000_0' to file:/content/wordcount/output2\n",
            "2024-05-06 14:15:01,536 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-05-06 14:15:01,536 INFO mapred.Task: Task 'attempt_local1256038990_0001_r_000000_0' done.\n",
            "2024-05-06 14:15:01,537 INFO mapred.Task: Final Counters for attempt_local1256038990_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=89170\n",
            "\t\tFILE: Number of bytes written=804550\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=746\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=371195904\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-05-06 14:15:01,537 INFO mapred.LocalJobRunner: Finishing task: attempt_local1256038990_0001_r_000000_0\n",
            "2024-05-06 14:15:01,538 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-06 14:15:01,780 INFO mapreduce.Job: Job job_local1256038990_0001 running in uber mode : false\n",
            "2024-05-06 14:15:01,781 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-06 14:15:01,783 INFO mapreduce.Job: Job job_local1256038990_0001 completed successfully\n",
            "2024-05-06 14:15:01,793 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=119402\n",
            "\t\tFILE: Number of bytes written=1551559\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1492\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=742391808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-05-06 14:15:01,793 INFO streaming.StreamJob: Output directory: wordcount/output2\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output2 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file swap_keyval.sh \\\n",
        "  -input wordcount/output \\\n",
        "  -output wordcount/output2 \\\n",
        "  -mapper swap_keyval.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGa-qTjkD3Fd"
      },
      "source": [
        "Check the output on HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDj-FrWZD3Fd",
        "outputId": "8cee81e6-eb2c-4e85-c1a5-2d8157306448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-05-06 14:15 wordcount/output2/_SUCCESS\n",
            "-rw-r--r--   1 root root      27860 2024-05-06 14:15 wordcount/output2/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZs75WfwD3Fd",
        "outputId": "ae164fbd-e1e5-4be4-ff0c-858329d1fc5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t!!n.frames[t];\n",
            "1\tüberraschen.\n",
            "1\tüber\n",
            "1\t©\n",
            "1\t},\n",
            "1\t}();\n",
            "1\t}(),\n",
            "1\t{};\n",
            "1\ty(){E[\"default\"].debug(\"User\n",
            "1\ty(),j(),void(ne=D());case\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwU4MGGVD3Fe"
      },
      "source": [
        "Mapper uses by default ascending order to sort by key. We could have changed that with an option but for now let's look at the end of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COXboJaHD3Fe",
        "outputId": "21051b06-e2aa-4f7b-c84c-9d0cad509176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\t==\n",
            "7\t:\n",
            "7\tdie\n",
            "7\t?\n",
            "7\tif\n",
            "7\ttypeof\n",
            "8\t0\n",
            "8\tn(e){return\n",
            "9\te&&e.__esModule?e:{\"default\":e}}function\n",
            "9\tr\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|tail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QRPxpgtD3Fe"
      },
      "source": [
        "### Configure sort with `KeyFieldBasedComparator` <a name=\"KeyFieldBasedComparator\"></a>\n",
        "\n",
        "In general, we can determine how mappers are going to sort their output by configuring the comparator directive to use the special class [`KeyFieldBasedComparator`](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html)\n",
        "<html><pre>-D mapreduce.job.output.key.comparator.class=\\\n",
        "    org.apache.hadoop.mapred.lib.KeyFieldBasedComparator</pre></html>\n",
        "    \n",
        "This class has some options similar to the Unix `sort`(`-n` to sort numerically, `-r` for reverse sorting, `-k pos1[,pos2]` for specifying fields to sort by).\n",
        "\n",
        "Let us see the comparator in action on our data to get the desired result. Note that this time we are removing `output2` because we're running the second mapreduce job again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yckLOf7dD3Fe",
        "outputId": "bee451fa-2363-4e8e-eaf9-b0187a5677f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output2\n",
            "packageJobJar: [swap_keyval.sh] [] /tmp/streamjob12970272091197247414.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-06 14:15:12,970 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-05-06 14:15:13,839 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 14:15:14,033 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 14:15:14,033 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 14:15:14,075 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:15:14,306 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 14:15:14,340 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 14:15:14,705 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1260432321_0001\n",
            "2024-05-06 14:15:14,705 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 14:15:15,144 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local1260432321_0001_d618dfe7-a746-461c-991c-9f524cb4a8c4/swap_keyval.sh\n",
            "2024-05-06 14:15:15,253 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 14:15:15,255 INFO mapreduce.Job: Running job: job_local1260432321_0001\n",
            "2024-05-06 14:15:15,266 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 14:15:15,269 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 14:15:15,276 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:15:15,276 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:15:15,383 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 14:15:15,389 INFO mapred.LocalJobRunner: Starting task: attempt_local1260432321_0001_m_000000_0\n",
            "2024-05-06 14:15:15,453 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:15:15,453 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:15:15,485 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:15:15,505 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n",
            "2024-05-06 14:15:15,522 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-05-06 14:15:15,596 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-06 14:15:15,597 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-06 14:15:15,597 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-06 14:15:15,597 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-06 14:15:15,597 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-06 14:15:15,603 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-06 14:15:15,615 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n",
            "2024-05-06 14:15:15,622 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-05-06 14:15:15,626 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-05-06 14:15:15,627 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-05-06 14:15:15,628 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-05-06 14:15:15,634 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-05-06 14:15:15,635 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-05-06 14:15:15,640 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-05-06 14:15:15,640 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-05-06 14:15:15,641 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-05-06 14:15:15,641 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-05-06 14:15:15,642 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-06 14:15:15,643 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-05-06 14:15:15,670 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:15:15,671 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:15:15,672 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 14:15:15,689 INFO streaming.PipeMapRed: Records R/W=746/1\n",
            "2024-05-06 14:15:15,741 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 14:15:15,742 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 14:15:15,746 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 14:15:15,746 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-06 14:15:15,746 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-06 14:15:15,747 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n",
            "2024-05-06 14:15:15,747 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n",
            "2024-05-06 14:15:15,770 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-06 14:15:15,792 INFO mapred.Task: Task:attempt_local1260432321_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:15:15,795 INFO mapred.LocalJobRunner: Records R/W=746/1\n",
            "2024-05-06 14:15:15,795 INFO mapred.Task: Task 'attempt_local1260432321_0001_m_000000_0' done.\n",
            "2024-05-06 14:15:15,807 INFO mapred.Task: Final Counters for attempt_local1260432321_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=30232\n",
            "\t\tFILE: Number of bytes written=747919\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=746\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=344981504\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "2024-05-06 14:15:15,807 INFO mapred.LocalJobRunner: Finishing task: attempt_local1260432321_0001_m_000000_0\n",
            "2024-05-06 14:15:15,807 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 14:15:15,811 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-06 14:15:15,812 INFO mapred.LocalJobRunner: Starting task: attempt_local1260432321_0001_r_000000_0\n",
            "2024-05-06 14:15:15,822 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 14:15:15,822 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 14:15:15,822 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 14:15:15,826 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6636ca87\n",
            "2024-05-06 14:15:15,828 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:15:15,873 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-06 14:15:15,887 INFO reduce.EventFetcher: attempt_local1260432321_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-06 14:15:15,965 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1260432321_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n",
            "2024-05-06 14:15:15,969 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local1260432321_0001_m_000000_0\n",
            "2024-05-06 14:15:15,971 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n",
            "2024-05-06 14:15:15,975 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-06 14:15:15,976 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:15:15,977 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-06 14:15:15,984 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:15:15,984 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n",
            "2024-05-06 14:15:15,992 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-06 14:15:15,993 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n",
            "2024-05-06 14:15:15,994 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-06 14:15:15,994 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 14:15:15,995 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n",
            "2024-05-06 14:15:15,996 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:15:16,026 INFO mapred.Task: Task:attempt_local1260432321_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 14:15:16,028 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 14:15:16,028 INFO mapred.Task: Task attempt_local1260432321_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-06 14:15:16,030 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1260432321_0001_r_000000_0' to file:/content/wordcount/output2\n",
            "2024-05-06 14:15:16,032 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-05-06 14:15:16,032 INFO mapred.Task: Task 'attempt_local1260432321_0001_r_000000_0' done.\n",
            "2024-05-06 14:15:16,035 INFO mapred.Task: Final Counters for attempt_local1260432321_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=89170\n",
            "\t\tFILE: Number of bytes written=805460\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=746\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=344981504\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-05-06 14:15:16,036 INFO mapred.LocalJobRunner: Finishing task: attempt_local1260432321_0001_r_000000_0\n",
            "2024-05-06 14:15:16,036 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-06 14:15:16,265 INFO mapreduce.Job: Job job_local1260432321_0001 running in uber mode : false\n",
            "2024-05-06 14:15:16,266 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-06 14:15:16,268 INFO mapreduce.Job: Job job_local1260432321_0001 completed successfully\n",
            "2024-05-06 14:15:16,277 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=119402\n",
            "\t\tFILE: Number of bytes written=1553379\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1492\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=689963008\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-05-06 14:15:16,277 INFO streaming.StreamJob: Output directory: wordcount/output2\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rmr wordcount/output2 2>/dev/null\n",
        "comparator_class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\n",
        "mapred streaming \\\n",
        "  -D mapreduce.job.output.key.comparator.class=$comparator_class \\\n",
        "  -D mapreduce.partition.keycomparator.options=-nr \\\n",
        "  -file swap_keyval.sh \\\n",
        "  -input wordcount/output \\\n",
        "  -output wordcount/output2 \\\n",
        "  -mapper swap_keyval.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh2cIb9mD3Fe",
        "outputId": "11c6909c-52da-4c70-b116-354ebf724153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-05-06 14:15 wordcount/output2/_SUCCESS\n",
            "-rw-r--r--   1 root root      27860 2024-05-06 14:15 wordcount/output2/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWxjdQ7hD3Fe",
        "outputId": "4f935571-6cea-41b3-95f7-323724d6ca4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\t=\n",
            "22\t{\n",
            "22\tvar\n",
            "19\t&&\n",
            "13\tstrict\";function\n",
            "12\tnot\n",
            "12\t}\n",
            "12\tin\n",
            "10\tto\n",
            "9\te&&e.__esModule?e:{\"default\":e}}function\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APPySwzzD3Ff"
      },
      "source": [
        "Now we get the output in the desired order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alzQgJAnD3Fi"
      },
      "source": [
        "### Specifying Configuration Variables with the -D Option <a name=\"configuration_variables\"></a>\n",
        "\n",
        "With the `-D` option it is possible to override options set in the default configuration file [`mapred_default.xml`](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)\n",
        "(see the [Apache Hadoop documentation](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Specifying_Configuration_Variables_with_the_-D_Option)).\n",
        "\n",
        "One option that might come handy when dealing with out-of-memory issues in the sorting phase is the size in MB of the memory reserved for sorting `mapreduce.task.io.sort.mb`:\n",
        " <html>\n",
        "    <pre>-D mapreduce.task.io.sort.mb=512\n",
        "    </pre>\n",
        " </html>\n",
        "\n",
        " **Note:** the maximum value for `mapreduce.task.io.sort.mb` is 2047.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wYKD9aID3Fi"
      },
      "source": [
        "## What is word count useful for? <a name=\"wordcount\"></a>\n",
        "Counting the frequencies of words is at the basis of _indexing_ and it facilitates the retrieval of relevant documents in search engines."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}