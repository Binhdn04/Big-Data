{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "2938e1da-e280-4b70-bfea-5173e0f776aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "4cb8c0bb-e37b-481d-ec45-cfd34741bbd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "1f9b9fb8-cf99-475d-a046-38b64b668657"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "8e935817-57d9-4054-cf7f-dbfca1ccffc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-05-06 13:58:43,366 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 13:58:43,683 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 13:58:43,684 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 13:58:43,710 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 13:58:44,062 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 13:58:44,100 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 13:58:44,597 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1558234504_0001\n",
            "2024-05-06 13:58:44,597 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 13:58:44,879 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 13:58:44,881 INFO mapreduce.Job: Running job: job_local1558234504_0001\n",
            "2024-05-06 13:58:44,890 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 13:58:44,896 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 13:58:44,904 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:58:44,904 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:58:44,965 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 13:58:44,972 INFO mapred.LocalJobRunner: Starting task: attempt_local1558234504_0001_m_000000_0\n",
            "2024-05-06 13:58:45,019 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:58:45,023 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:58:45,057 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 13:58:45,069 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-05-06 13:58:45,087 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-05-06 13:58:45,167 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-06 13:58:45,167 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-06 13:58:45,167 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-06 13:58:45,167 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-06 13:58:45,168 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-06 13:58:45,174 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-06 13:58:45,182 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-05-06 13:58:45,191 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-05-06 13:58:45,195 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-05-06 13:58:45,195 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-05-06 13:58:45,196 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-05-06 13:58:45,196 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-05-06 13:58:45,197 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-05-06 13:58:45,199 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-05-06 13:58:45,200 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-05-06 13:58:45,200 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-05-06 13:58:45,201 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-05-06 13:58:45,202 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-06 13:58:45,202 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-05-06 13:58:45,236 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 13:58:45,238 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-05-06 13:58:45,241 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 13:58:45,241 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 13:58:45,247 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 13:58:45,247 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-06 13:58:45,247 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-06 13:58:45,247 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-05-06 13:58:45,248 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-05-06 13:58:45,259 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-06 13:58:45,277 INFO mapred.Task: Task:attempt_local1558234504_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 13:58:45,281 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-05-06 13:58:45,282 INFO mapred.Task: Task 'attempt_local1558234504_0001_m_000000_0' done.\n",
            "2024-05-06 13:58:45,294 INFO mapred.Task: Final Counters for attempt_local1558234504_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=857635\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-05-06 13:58:45,294 INFO mapred.LocalJobRunner: Finishing task: attempt_local1558234504_0001_m_000000_0\n",
            "2024-05-06 13:58:45,295 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 13:58:45,300 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-06 13:58:45,301 INFO mapred.LocalJobRunner: Starting task: attempt_local1558234504_0001_r_000000_0\n",
            "2024-05-06 13:58:45,337 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:58:45,338 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:58:45,338 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 13:58:45,343 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@133c07ba\n",
            "2024-05-06 13:58:45,345 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 13:58:45,389 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-06 13:58:45,399 INFO reduce.EventFetcher: attempt_local1558234504_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-06 13:58:45,448 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1558234504_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-05-06 13:58:45,460 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local1558234504_0001_m_000000_0\n",
            "2024-05-06 13:58:45,463 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-05-06 13:58:45,467 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-06 13:58:45,468 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 13:58:45,468 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-06 13:58:45,475 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 13:58:45,476 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-05-06 13:58:45,477 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-06 13:58:45,478 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-05-06 13:58:45,479 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-06 13:58:45,479 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 13:58:45,480 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-05-06 13:58:45,481 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 13:58:45,490 INFO mapred.Task: Task:attempt_local1558234504_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 13:58:45,491 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 13:58:45,491 INFO mapred.Task: Task attempt_local1558234504_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-06 13:58:45,493 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1558234504_0001_r_000000_0' to file:/content/my_output\n",
            "2024-05-06 13:58:45,501 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-05-06 13:58:45,501 INFO mapred.Task: Task 'attempt_local1558234504_0001_r_000000_0' done.\n",
            "2024-05-06 13:58:45,503 INFO mapred.Task: Final Counters for attempt_local1558234504_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141992\n",
            "\t\tFILE: Number of bytes written=857685\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-05-06 13:58:45,503 INFO mapred.LocalJobRunner: Finishing task: attempt_local1558234504_0001_r_000000_0\n",
            "2024-05-06 13:58:45,503 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-06 13:58:45,887 INFO mapreduce.Job: Job job_local1558234504_0001 running in uber mode : false\n",
            "2024-05-06 13:58:45,889 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-06 13:58:45,890 INFO mapreduce.Job: Job job_local1558234504_0001 completed successfully\n",
            "2024-05-06 13:58:45,904 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283906\n",
            "\t\tFILE: Number of bytes written=1715320\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=725614592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-05-06 13:58:45,904 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "d0262acb-f71c-4579-e62f-eea740de99f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "2a151a4d-a963-48f8-bc61-45c20eae2494"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-05-06 13:58 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-05-06 13:58 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "434bbfb3-f58c-42b6-c88c-dfcae7ba1a87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 May  6 13:58 part-00000\n",
            "-rw-r--r-- 1 root root  0 May  6 13:58 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "5eef3b31-8dea-4e56-81e7-c8592ea91f0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "44ebcbd8-cf1b-4e39-a2e8-c2a4baea977a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-06 13:58:53,470 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "74a79b53-0df3-401f-e231-defe296b9beb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-06 13:58:55,517 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-05-06 13:58:57,921 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 13:58:58,122 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 13:58:58,122 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 13:58:58,156 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 13:58:58,483 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 13:58:58,526 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 13:58:58,891 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local3260370_0001\n",
            "2024-05-06 13:58:58,891 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 13:58:59,200 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 13:58:59,202 INFO mapreduce.Job: Running job: job_local3260370_0001\n",
            "2024-05-06 13:58:59,212 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 13:58:59,215 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 13:58:59,225 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:58:59,229 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:58:59,304 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 13:58:59,311 INFO mapred.LocalJobRunner: Starting task: attempt_local3260370_0001_m_000000_0\n",
            "2024-05-06 13:58:59,369 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:58:59,370 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:58:59,414 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 13:58:59,426 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-05-06 13:58:59,445 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-05-06 13:58:59,541 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-06 13:58:59,541 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-06 13:58:59,541 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-06 13:58:59,541 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-06 13:58:59,541 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-06 13:58:59,547 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-06 13:58:59,557 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 13:58:59,557 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-06 13:58:59,557 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-06 13:58:59,558 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-05-06 13:58:59,558 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-05-06 13:58:59,568 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-06 13:58:59,589 INFO mapred.Task: Task:attempt_local3260370_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 13:58:59,592 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-05-06 13:58:59,592 INFO mapred.Task: Task 'attempt_local3260370_0001_m_000000_0' done.\n",
            "2024-05-06 13:58:59,628 INFO mapred.Task: Final Counters for attempt_local3260370_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=845197\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-05-06 13:58:59,628 INFO mapred.LocalJobRunner: Finishing task: attempt_local3260370_0001_m_000000_0\n",
            "2024-05-06 13:58:59,630 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 13:58:59,635 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-06 13:58:59,636 INFO mapred.LocalJobRunner: Starting task: attempt_local3260370_0001_r_000000_0\n",
            "2024-05-06 13:58:59,652 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:58:59,652 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:58:59,653 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 13:58:59,660 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5da14c83\n",
            "2024-05-06 13:58:59,662 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 13:58:59,689 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-06 13:58:59,691 INFO reduce.EventFetcher: attempt_local3260370_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-06 13:58:59,746 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local3260370_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-05-06 13:58:59,750 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local3260370_0001_m_000000_0\n",
            "2024-05-06 13:58:59,755 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-05-06 13:58:59,759 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-06 13:58:59,761 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 13:58:59,761 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-06 13:58:59,771 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 13:58:59,772 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-05-06 13:58:59,773 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-06 13:58:59,774 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-05-06 13:58:59,775 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-06 13:58:59,775 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-06 13:58:59,776 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-05-06 13:58:59,777 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 13:58:59,786 INFO mapred.Task: Task:attempt_local3260370_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 13:58:59,788 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-06 13:58:59,788 INFO mapred.Task: Task attempt_local3260370_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-06 13:58:59,792 INFO output.FileOutputCommitter: Saved output of task 'attempt_local3260370_0001_r_000000_0' to file:/content/my_output\n",
            "2024-05-06 13:58:59,793 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-05-06 13:58:59,794 INFO mapred.Task: Task 'attempt_local3260370_0001_r_000000_0' done.\n",
            "2024-05-06 13:58:59,795 INFO mapred.Task: Final Counters for attempt_local3260370_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142006\n",
            "\t\tFILE: Number of bytes written=845255\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-05-06 13:58:59,795 INFO mapred.LocalJobRunner: Finishing task: attempt_local3260370_0001_r_000000_0\n",
            "2024-05-06 13:58:59,795 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-06 13:59:00,210 INFO mapreduce.Job: Job job_local3260370_0001 running in uber mode : false\n",
            "2024-05-06 13:59:00,211 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-06 13:59:00,212 INFO mapreduce.Job: Job job_local3260370_0001 completed successfully\n",
            "2024-05-06 13:59:00,223 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283920\n",
            "\t\tFILE: Number of bytes written=1690452\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=864026624\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-05-06 13:59:00,223 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "3e4510be-8284-4759-b1cc-8d4ee4565512"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "fb3eb27a-708c-4b49-9f40-9e4d8d153ec7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "28e62e7f-974a-42f1-be08-0c77b4d0d39f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-06 13:59:05,602 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-05-06 13:59:07,915 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 13:59:08,131 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 13:59:08,132 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 13:59:08,152 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 13:59:08,453 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 13:59:08,480 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 13:59:08,870 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1673113466_0001\n",
            "2024-05-06 13:59:08,870 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 13:59:09,154 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 13:59:09,156 INFO mapreduce.Job: Running job: job_local1673113466_0001\n",
            "2024-05-06 13:59:09,166 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 13:59:09,169 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 13:59:09,177 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:59:09,177 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:59:09,248 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 13:59:09,254 INFO mapred.LocalJobRunner: Starting task: attempt_local1673113466_0001_m_000000_0\n",
            "2024-05-06 13:59:09,305 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:59:09,308 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:59:09,346 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 13:59:09,361 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-05-06 13:59:09,379 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-05-06 13:59:09,402 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 13:59:09,415 INFO mapred.Task: Task:attempt_local1673113466_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 13:59:09,420 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 13:59:09,420 INFO mapred.Task: Task attempt_local1673113466_0001_m_000000_0 is allowed to commit now\n",
            "2024-05-06 13:59:09,430 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1673113466_0001_m_000000_0' to file:/content/my_output\n",
            "2024-05-06 13:59:09,435 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-05-06 13:59:09,436 INFO mapred.Task: Task 'attempt_local1673113466_0001_m_000000_0' done.\n",
            "2024-05-06 13:59:09,448 INFO mapred.Task: Final Counters for attempt_local1673113466_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855489\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=341835776\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-05-06 13:59:09,449 INFO mapred.LocalJobRunner: Finishing task: attempt_local1673113466_0001_m_000000_0\n",
            "2024-05-06 13:59:09,450 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 13:59:10,164 INFO mapreduce.Job: Job job_local1673113466_0001 running in uber mode : false\n",
            "2024-05-06 13:59:10,166 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-05-06 13:59:10,185 INFO mapreduce.Job: Job job_local1673113466_0001 completed successfully\n",
            "2024-05-06 13:59:10,193 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855489\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=341835776\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-05-06 13:59:10,193 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "394a083e-4f65-4283-f1a7-3bc4df3bcc3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "04733253-ef1c-4de7-c100-2dfd60db3255"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-06 13:59:14,654 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-05-06 13:59:18,327 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 13:59:18,552 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 13:59:18,552 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 13:59:18,585 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 13:59:18,967 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-06 13:59:19,005 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-06 13:59:19,364 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local608079995_0001\n",
            "2024-05-06 13:59:19,365 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-06 13:59:19,684 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-06 13:59:19,686 INFO mapreduce.Job: Running job: job_local608079995_0001\n",
            "2024-05-06 13:59:19,702 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-06 13:59:19,705 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-06 13:59:19,715 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:59:19,715 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:59:19,779 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-06 13:59:19,789 INFO mapred.LocalJobRunner: Starting task: attempt_local608079995_0001_m_000000_0\n",
            "2024-05-06 13:59:19,832 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-06 13:59:19,832 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-06 13:59:19,861 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-06 13:59:19,878 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-05-06 13:59:19,899 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-05-06 13:59:19,921 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-05-06 13:59:19,934 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-05-06 13:59:19,937 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-05-06 13:59:19,938 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-05-06 13:59:19,938 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-05-06 13:59:19,938 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-05-06 13:59:19,939 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-05-06 13:59:19,942 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-05-06 13:59:19,943 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-05-06 13:59:19,943 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-05-06 13:59:19,944 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-05-06 13:59:19,945 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-06 13:59:19,946 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-05-06 13:59:19,974 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-06 13:59:19,982 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-05-06 13:59:19,983 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-06 13:59:19,983 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-06 13:59:19,987 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 13:59:19,999 INFO mapred.Task: Task:attempt_local608079995_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-06 13:59:20,004 INFO mapred.LocalJobRunner: \n",
            "2024-05-06 13:59:20,005 INFO mapred.Task: Task attempt_local608079995_0001_m_000000_0 is allowed to commit now\n",
            "2024-05-06 13:59:20,009 INFO output.FileOutputCommitter: Saved output of task 'attempt_local608079995_0001_m_000000_0' to file:/content/my_output\n",
            "2024-05-06 13:59:20,011 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-05-06 13:59:20,011 INFO mapred.Task: Task 'attempt_local608079995_0001_m_000000_0' done.\n",
            "2024-05-06 13:59:20,024 INFO mapred.Task: Final Counters for attempt_local608079995_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=216006656\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-05-06 13:59:20,024 INFO mapred.LocalJobRunner: Finishing task: attempt_local608079995_0001_m_000000_0\n",
            "2024-05-06 13:59:20,025 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-06 13:59:20,700 INFO mapreduce.Job: Job job_local608079995_0001 running in uber mode : false\n",
            "2024-05-06 13:59:20,703 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-05-06 13:59:20,706 INFO mapreduce.Job: Job job_local608079995_0001 completed successfully\n",
            "2024-05-06 13:59:20,714 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=216006656\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-05-06 13:59:20,714 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "985e8e2f-b0f5-4c61-f386-3c87b5ad0c87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ]
}