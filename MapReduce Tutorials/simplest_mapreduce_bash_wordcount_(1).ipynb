{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iIw-8oyIy-f"
      },
      "source": [
        "# Simple distributed wordcount with MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT6lnV46Iy-g"
      },
      "source": [
        "Check that file `file.txt` exists, view size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vyHKMVBIy-g",
        "outputId": "f9e32671-7aba-4e93-d8c4-43dce85ed7f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'file.txt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls -hal file.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVxW55wJI-Js",
        "outputId": "fb86d5a4-ee16-488a-b4c9-78f7ed6a5689"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_LrmNl2JBbr",
        "outputId": "e7107f5f-8b09-4a55-f532-7faef0056849"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyDtjvG-JDTZ",
        "outputId": "49e32c32-3fb5-4ccb-f96b-f894afab8c6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73JxYdnIy-g"
      },
      "source": [
        "Copy file to HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Nt9PehYYIy-h",
        "outputId": "2fee94af-ebf5-46b6-f771-eb17b3c4740c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "put: `file.txt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -put -f file.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtztjvZrIy-h"
      },
      "source": [
        "Erase `result` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tpdbbm1MIy-h"
      },
      "outputs": [],
      "source": [
        "!hdfs dfs -rm -R result 2>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9rqcZQrIy-h"
      },
      "source": [
        "Run the bash wordcount command `wc` in parallel on the distributed file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99_t2pSKIy-h",
        "outputId": "b2df2fc3-92ba-481b-f111-afe3c4430b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-06 14:19:51,304 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-06 14:19:51,568 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-06 14:19:51,569 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-06 14:19:51,598 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-06 14:19:51,927 INFO mapreduce.JobSubmitter: Cleaning up the staging area file:/tmp/hadoop/mapred/staging/root2041750551/.staging/job_local2041750551_0001\n",
            "2024-05-06 14:19:51,935 ERROR streaming.StreamJob: Error Launching job : Input path does not exist: file:/content/file.txt\n",
            "Streaming Command Failed!\n"
          ]
        }
      ],
      "source": [
        "!mapred streaming \\\n",
        "  -input file.txt \\\n",
        "  -output result \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTPqmBGeIy-i"
      },
      "source": [
        "Check result of MapReduce job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmquUffXIy-i",
        "outputId": "bb6d7a4f-a8c2-48e8-c9de-3ce8090ac2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: `result/part*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat result/part*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5kCAxd7Iy-i"
      },
      "source": [
        "Check that the word count is correct by comparing with `wc` on local host (warning: do not try with too large files)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF-pVICoIy-i",
        "outputId": "d30a8406-eb64-4f78-8667-0f685004701f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wc: file.txt: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!wc file.txt"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}